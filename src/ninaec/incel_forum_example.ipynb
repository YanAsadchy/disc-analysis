{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The below [autoreload](https://ipython.org/ipython-doc/3/config/extensions/autoreload.html) extension ensures that if any locally imported python files change, the modules defined there are reloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-05-25T07:54:02.892690Z",
     "start_time": "2023-05-25T07:54:02.758420200Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The below imports [`here`](https://pypi.org/project/pyprojroot/), which allows one to refer to the root directory of the project in a consistent manner across execution environments. It then adds `here()` (the root directory) to the system path to ensure that we can load python modules defined in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-05-25T07:54:03.510193200Z",
     "start_time": "2023-05-25T07:54:02.893701400Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install hereutil flask_sqlalchemy\n",
    "import pandas as pd\n",
    "from hereutil import here, add_to_sys_path\n",
    "add_to_sys_path(here())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Having ensured that the root path of the project is in the system path, we can load common basis functions from [src/common_basis.py](/src/common_basis.py). The template assumes that functions useful for most work be defined in `common_basis.py`, whereas code useful for individual analyses is defined where needed.\n",
    "\n",
    "Naturally, if more refined organisation of common code is needed, one is also free to define whichever other modules one wants.\n",
    "\n",
    "The central object defined in `common_basis` is `con`, which is the [MariaDB](https://mariadb.com/) (MySQL) database connection (an [SQLAlchemy Connection](https://docs.sqlalchemy.org/en/14/core/connections.html)) through which both ready data is accessed, as well as new data stored for others to reuse. Below, you will see both how to use con to store data in the database, as well as how query it.\n",
    "\n",
    "The details of the database connection are stored in [`db_params.yaml`](/db_params.yaml). The password is given separately. **DO NOT INCLUDE THE PASSWORD IN ANY CODE YOU COMMIT TO GITHUB**. If running this notebook, it will ask for the password the first time you run it, and then store it separately in your keyring. This requires a working keyring implementation on your system. Consult the [`keyring`](https://pypi.org/project/keyring/) package documentation if you have problems.  If you cannot get it to work, a second option is to create a `db_secret.yaml` file in the project root directory with `db_pass: [PASSWORD]` as the content. This file is already set to be ignored by Git so it wouldn't accidentally get included in a commit, but still, if you do this, **DON'T MAKE THE MISTAKE OF COMMITTING THE FILE TO GITHUB**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-05-25T07:54:04.403866100Z",
     "start_time": "2023-05-25T07:54:03.512210600Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.common_basis import *\n",
    "\n",
    "eng, con = get_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Example of reading tweets from lynching_tweets_a table\n",
    "\n",
    "The following code reads a random sample of tweets from lynching_tweets_a table with following specifications:\n",
    "\n",
    "- **keyword**: each tweet returned by the query has the keyword as a substring\n",
    "- **n** : upper limit of how many tweets are fetched\n",
    "- **start_date**: fetches tweets where date_created_at is the start_date or later\n",
    "- **end_date**: looks for tweets until the day end_date - 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-05-25T07:55:07.275056800Z",
     "start_time": "2023-05-25T07:54:04.412967Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  \n",
      "SELECT *\n",
      "FROM lynching_tweets_a\n",
      "WHERE MATCH(text) AGAINST('india' IN BOOLEAN MODE) \n",
      "AND created_at BETWEEN '2020-02-01' AND '2023-02-05'\n",
      "ORDER BY RAND()\n",
      "LIMIT 100;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import text\n",
    "\n",
    "keyword = \"india\"\n",
    "n = 100\n",
    "start_date = '2020-02-01'\n",
    "end_date = '2023-02-05'\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT *\n",
    "FROM lynching_tweets_a\n",
    "WHERE MATCH(text) AGAINST('{keyword}' IN BOOLEAN MODE) \n",
    "AND created_at BETWEEN '{start_date}' AND '{end_date}'\n",
    "ORDER BY RAND()\n",
    "LIMIT {n};\n",
    "\"\"\"\n",
    "\n",
    "print(\"Query: \", query)\n",
    "\n",
    "#df = pd.read_sql(text(query), con)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T07:55:07.459232900Z",
     "start_time": "2023-05-25T07:55:07.285356800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0     RT @SpiritOfCongres: India under BJP:\\n\\n⚫Riot...\n1     @azharulhaq @cjwerleman @ShiningSadaf @UN Indi...\n2     RT @bainjal: New normal in new India cops watc...\n3     RT @Shubham_fd: Lynching of Brahmins by Dalits...\n4     Dumbledore’s Army has spoken. Trans Women are ...\n                            ...                        \n95    RT @imMAK02: Muslims are systematically target...\n96    @FaroghYusuf @vandurocks Because I am a human....\n97    RT @imMAK02: Hindutva Nazis are trending \"I st...\n98    RT @BachiBashith: Bjp leader nation wide calle...\n99    RT @Aakashhassan: Kashmiri boy who was lynched...\nName: text, Length: 100, dtype: object"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show text column of the result \n",
    "df['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Topic Modelling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy\n",
    "import spacy\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import TfidfModel\n",
    "# vis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def prep_data(data) -> list:\n",
    "    \"\"\"\n",
    "    Wandelt die JSON Struktur des Datensets um in eine Liste.\n",
    "    :param data: Zu bearbeitendes Datenset.\n",
    "    :return: Liste mit Post-Inhalten aus dem Datenset.\n",
    "    \"\"\"\n",
    "    text_list = []\n",
    "    post_content = ''\n",
    "    for idx, text in enumerate(data):\n",
    "        post_content = data[idx]['content']\n",
    "        text_list.append(post_content)\n",
    "    return text_list\n",
    "\n",
    "def lemmatization(texts, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "    \"\"\"\n",
    "    Filtert die Stopwörter aus den Post Texten heraus.\n",
    "    :param texts: Liste an Post-Texten aus dem Datenset.\n",
    "    :param allowed_postags: Wörter die nicht Herausgefiltert werden, z.B. Nomen.\n",
    "    :return: Lemmatized texts, daher die gefilterten Texte.\n",
    "    \"\"\"\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"]) # Quelle: https://github.com/explosion/spaCy/issues/7453\n",
    "    texts_out = []\n",
    "    for text in texts:\n",
    "        if text != None:\n",
    "            doc = nlp(text)\n",
    "            new_text = []\n",
    "            for token in doc:\n",
    "                if token.pos_ in allowed_postags:\n",
    "                    new_text.append(token.lemma_)\n",
    "\n",
    "            final = \" \".join(new_text)\n",
    "            texts_out.append(final)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return texts_out\n",
    "\n",
    "def gen_words(texts):\n",
    "    \"\"\"\n",
    "    Gensim spezifisches Preprocessing.\n",
    "    :param texts: Liste an Post-Texten aus dem Datenset.\n",
    "    :return: Verarbeitete Texte.\n",
    "    \"\"\"\n",
    "    final = []\n",
    "    for text in texts:\n",
    "        new = gensim.utils.simple_preprocess(text, deacc=True)\n",
    "        final.append(new)\n",
    "    return final\n",
    "\n",
    "def make_bigram(texts, bigram):\n",
    "    bigram = (bigram[doc] for doc in texts)\n",
    "    return bigram\n",
    "\n",
    "def make_trigram(texts, trigram, bigram):\n",
    "    trigram = (trigram[bigram[text]] for text in enumerate(texts))\n",
    "    return trigram"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T07:55:07.468377300Z",
     "start_time": "2023-05-25T07:55:07.460266200Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_topicmodelling(data, target_file_path):\n",
    "    \"\"\"\n",
    "    Basis Methode um Topic Modelling zu performen. Die Methode ist in folgende Schritte unterteilt: Vorbereitung der Daten, Preprocessing, Bilden von Bigrams und Trigrams,\n",
    "    Verarbeitung der Daten mit dem TFIDG Modell und Visualisierung mit pyLDAvis.\n",
    "    :type data: JSON-Objekt des zu bearbeitenden Datensets; wird in main.py geladen\n",
    "    :type target_file_path: Zielpfad für Speichern der Visualisierung aus pyLDAvis.\n",
    "    \"\"\"\n",
    "    # prepare data\n",
    "    data_posts = prep_data(data)\n",
    "\n",
    "    # Preprocessing - Herausfiltern von Stopwords und Transformation der Wörter in Grundform (Lemmatization)\n",
    "    stop = set(stopwords.words('english'))\n",
    "    lemmatized_texts = self.lemmatization(data_posts)\n",
    "    lemmatized_data = list(self.gen_words(lemmatized_texts))\n",
    "    print(\"Lemmetized Data Example:\", lemmatized_data[0])\n",
    "\n",
    "    # bigram and trigams\n",
    "    bigrams_phrases = gensim.models.Phrases(lemmatized_data, min_count=5, threshold=100)\n",
    "    trigram_phrases = gensim.models.Phrases(bigrams_phrases[lemmatized_data], threshold=100)\n",
    "\n",
    "    bigram = gensim.models.phrases.Phraser(bigrams_phrases)\n",
    "    trigram = gensim.models.phrases.Phraser(trigram_phrases)\n",
    "\n",
    "    trigram_total = (trigram[bigram[lemmatized_data[i]]] for i in range(len(lemmatized_data)))\n",
    "    trigram_total = list(trigram_total)\n",
    "\n",
    "    id2word = corpora.Dictionary(trigram_total)\n",
    "    print(\"id2word - Unique Tokens Examples:\", id2word)\n",
    "    corpus = [id2word.doc2bow(text) for text in trigram_total]\n",
    "\n",
    "    # Verarbeitung der Daten mit tfidf\n",
    "    tfidf = TfidfModel(corpus, id2word=id2word)\n",
    "\n",
    "    low_value = 0.03\n",
    "    words = []\n",
    "    words_missing_in_tfidf = []\n",
    "\n",
    "    for i in range(0, len(corpus)):\n",
    "        bow = corpus[i]\n",
    "        low_value_words = []  # reinitialize to be safe. You can skip this.\n",
    "        tfidf_ids = [id for id, value in tfidf[bow]]\n",
    "        bow_ids = [id for id, value in bow]\n",
    "        low_value_words = [id for id, value in tfidf[bow] if value < low_value]\n",
    "        words_missing_in_tfidf = [id for id in bow_ids if\n",
    "                                  id not in tfidf_ids]  # The words with tf-idf socre 0 will be missing\n",
    "\n",
    "        new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tfidf]\n",
    "\n",
    "        # reassign\n",
    "        corpus[i] = new_bow\n",
    "    # Bereitstellung des LDA Model\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                                id2word=id2word,\n",
    "                                                num_topics=20,\n",
    "                                                random_state=100,\n",
    "                                                update_every=1,\n",
    "                                                chunksize=100,\n",
    "                                                passes=10,\n",
    "                                                alpha=\"auto\")\n",
    "\n",
    "    # Visualisierung und Speichern\n",
    "    vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word, mds=\"mmds\", R=30)\n",
    "    pyLDAvis.save_html(vis, target_file_path)\n",
    "\n",
    "# https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#1introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-25T07:55:07.514741100Z",
     "start_time": "2023-05-25T07:55:07.470391900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-25T07:55:07.515739600Z",
     "start_time": "2023-05-25T07:55:07.485957700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-25T07:55:07.517284700Z",
     "start_time": "2023-05-25T07:55:07.504234900Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
